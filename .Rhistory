setwd("~/Documents/HU Berlin/WI 1516/refugeestest/IS refugees")
data.refugees<-read.csv(file ="Data/Data_refugees_nachgeladen040116.csv")
score.sentiment = function(sentences, pos.words, neg.words, exc.words, .progress='none')
{
require(plyr)
require(stringr)
# we got a vector of sentences. plyr will handle a list or a vector as an "l" for us
# we want a simple array of scores back, so we use "l" + "a" + "ply" = laply:
scores = laply(sentences, function(sentence, pos.words, neg.words) {
# clean up sentences with R's regex-driven global substitute, gsub():
sentence = gsub('[[:punct:]]', '', sentence)
sentence = gsub('[[:cntrl:]]', '', sentence)
sentence = gsub('\\d+', '', sentence)
# and convert to lower case:
sentence = tolower(sentence)
# split into words. str_split is in the stringr package
word.list = str_split(sentence, '\\s+')
# sometimes a list() is one level of hierarchy too much
words = unlist(word.list)
# exclude stop words
check <- match(words,exc.words)
exc.list <-!is.na(check)
words <-words[!exc.list]
# compare our words to the dictionaries of positive & negative terms
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
# match() returns the position of the matched term or NA
# we just want a TRUE/FALSE:
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
# and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words, .progress=.progress )
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
data_for_sentiment<-data.refugees[,c(13,18)]
gsub("#|@|\n|/|\\|❤|/�", " ",data_for_sentiment$caption[13])
gsub("#|@|\n|/|\\|\❤|\�", " ",data_for_sentiment$caption[13])
data_for_sentiment_cleaned<-gsub("#|@|\n|/|\\|❤|�", " ",data_for_sentiment$caption[13])
gsub("#|@|\n|/|\\|❤|�", " ",data_for_sentiment$caption[13])
data_for_sentiment_cleaned<-gsub("#|@|\n|/|\\|[❤]|�", " ",data_for_sentiment$caption[13])
data_for_sentiment_cleaned
data_for_sentiment_cleaned<-gsub("#|@|\n|/|\\|/❤|�", " ",data_for_sentiment$caption[13])
data_for_sentiment_cleaned
data_for_sentiment_cleaned<-gsub("#|@|\n|/|\\|❤", " ",data_for_sentiment$caption[13])
data_for_sentiment_cleaned
data_for_sentiment_cleaned<-gsub("#|@|\n|/|❤"," ",data_for_sentiment$caption[13])
data_for_sentiment_cleaned
data_for_sentiment_cleaned<-gsub("#|@|\n|/|❤|\\\\"," ",data_for_sentiment$caption[13])
data_for_sentiment_cleaned
data_for_sentiment_cleaned<-gsub("#|@|\n|/|❤|([\\])"," ",data_for_sentiment$caption[13])
data_for_sentiment_cleaned
data_for_sentiment_cleaned<-gsub("#|@|\n|/|❤|([\])"," ",data_for_sentiment$caption[13])
data_for_sentiment_cleaned<-gsub("#|@|\n|/|❤|([\\])"," ",data_for_sentiment$caption[13])
data_for_sentiment_cleaned
data_for_sentiment_cleaned<-gsub("#|@|\n|/|❤|([\\])"," ",data_for_sentiment$caption[13], fixed=TRUE)
data_for_sentiment_cleaned<-gsub("#|@|\n|/|❤|\\"," ",data_for_sentiment$caption[13], fixed=TRUE)
data_for_sentiment_cleaned<-gsub("#|@|\n|/|❤"," ",data_for_sentiment$caption[13], fixed=TRUE)
data_for_sentiment_cleaned<-gsub("#|@|\n|/|❤"," ",data_for_sentiment$caption[13])
data_for_sentiment_cleaned
data_for_sentiment_cleaned<-gsub("#|@|\n|/|❤|\\\\"," ",data_for_sentiment$caption[13])
data_for_sentiment_cleaned
data_for_sentiment_cleaned<-gsub("#|@|\n|/|❤|[\\\\]"," ",data_for_sentiment$caption[13])
data_for_sentiment_cleaned
analysis<-score.sentiment(data_for_sentiment_cleaned[1:13],positive_words,negative_words, exc.words=NULL)
x <- "a1~!@#$%^&*(){}_+:\"<>?,./;'[]-=" #or whatever
data_for_sentiment_cleaned<-gsub("#|@|\n|/|❤"," ",data_for_sentiment$caption[13])
x <- "a1~!@#$%^&*(){}_+:\"<>?,./;'[]-=" #or whatever
a<-gsub(x," ",data_for_sentiment_cleaned)
x <- "a1~!@#$%^&*()_+:\"<>?,./;'[]-=" #or whatever
a<-gsub(x," ",data_for_sentiment_cleaned)
a<-gsub("[^[:alnum:][:blank:]+?&/\\-]"," ",data_for_sentiment_cleaned)
a
a<-gsub("[^[:alnum:][:blank:]+\\]"," ",data_for_sentiment_cleaned)
a
a<-gsub("[[:alnum:][:blank:]+\\]"," ",data_for_sentiment_cleaned)
a
a<-gsub("[^[:alnum:][:blank:]+\\]"," ",data_for_sentiment_cleaned)
a
data_for_sentiment_cleaned<-gsub("#|@|\n|/|❤|[^[:alnum:][:blank:]+\\]"," ",data_for_sentiment$caption[13])
data_for_sentiment_cleaned
data_for_sentiment_cleaned<-gsub("#|@|\n|/|❤|[^[:alnum:][:blank:]+\\]"," ",data_for_sentiment$caption)
analysis<-score.sentiment(data_for_sentiment_cleaned[1:13],positive_words,negative_words, exc.words=NULL)
positive_words<-scan("positive_word_bank.txt", what = "character", comment.char=";")
negative_words<-scan("negative_word_bank.txt", what = "character", comment.char=";")
analysis<-score.sentiment(data_for_sentiment_cleaned[1:13],positive_words,negative_words, exc.words=NULL)
analysis<-score.sentiment(data_for_sentiment_cleaned,positive_words,negative_words, exc.words=NULL)
table(analysis$score)
hist(analysis$score)
analysis
head(analysis)
text<-data_for_sentiment_cleaned[13]
url <- paste0("Thttps://community-sentiment.p.mashape.com/",text,"/")
r <- GET(url)
library(httr)
r <- GET(url)
url <- paste0("https://community-sentiment.p.mashape.com/",text,"/")
r <- GET(url)
http_status(r)
url <- "https://community-sentiment.p.mashape.com/text/"
r <- GET(url, add_headers("X-Mashape-Key: b046W1AuLimshPLIBGkMKeHGwYw0p1RCYLJjsnyHjhot6bIwCy",
"Content-Type": "application/x-www-form-urlencoded",
"Accept": "application/json"))
r <- GET(url, add_headers(.headers=c("X-Mashape-Key: b046W1AuLimshPLIBGkMKeHGwYw0p1RCYLJjsnyHjhot6bIwCy",
"Content-Type": "application/x-www-form-urlencoded",
"Accept": "application/json")))
r <- POST(url, add_headers(.headers=c("X-Mashape-Key: b046W1AuLimshPLIBGkMKeHGwYw0p1RCYLJjsnyHjhot6bIwCy",
"Content-Type": "application/x-www-form-urlencoded",
"Accept": "application/json")))
r <- POST(url)
r
http_status(r)
r <- POST(url, add_headers("X-Mashape-Key: b046W1AuLimshPLIBGkMKeHGwYw0p1RCYLJjsnyHjhot6bIwCy",
"Content-Type": "application/x-www-form-urlencoded",
"Accept": "application/json"), accept_json())
r <- POST(url, add_headers("X-Mashape-Key: b046W1AuLimshPLIBGkMKeHGwYw0p1RCYLJjsnyHjhot6bIwCy",
"Content-Type: application/x-www-form-urlencoded",
"Accept: application/json"), accept_json())
http_status(r)
r
r <- POST(url, add_headers("X-Mashape-Key: b046W1AuLimshPLIBGkMKeHGwYw0p1RCYLJjsnyHjhot6bIwCy",
"Content-Type: application/x-www-form-urlencoded",
"Accept: application/json"), body = list("txt"=text), accept_json())
r
http_status(r)
r <- POST(url, add_headers("X-Mashape-Key: b046W1AuLimshPLIBGkMKeHGwYw0p1RCYLJjsnyHjhot6bIwCy",
"Content-Type: application/x-www-form-urlencoded",
"Accept: application/json"), query=list("txt"=text), accept_json())
r
text
POST(url, add_headers("X-Mashape-Key: b046W1AuLimshPLIBGkMKeHGwYw0p1RCYLJjsnyHjhot6bIwCy",
"Content-Type: application/x-www-form-urlencoded",
"Accept: application/json"), query=list("txt"=text), accept_json())
r <- POST(url, add_headers("X-Mashape-Key"= "b046W1AuLimshPLIBGkMKeHGwYw0p1RCYLJjsnyHjhot6bIwCy",
"Content-Type"="application/x-www-form-urlencoded",
"Accept"="application/json"), query=list("txt"=text), accept_json())
r
r <- POST(url, add_headers("X-Mashape-Key"= "b046W1AuLimshPLIBGkMKeHGwYw0p1RCYLJjsnyHjhot6bIwCy",
"Content-Type"="application/x-www-form-urlencoded",
"Accept"="application/json"), query=list("txt"=text), accept_json())
r
r <- POST(url, add_headers("X-Mashape-Key"= "b046W1AuLimshPLIBGkMKeHGwYw0p1RCYLJjsnyHjhot6bIwCy",
"Content-Type"="application/x-www-form-urlencoded",
"Accept"="application/json"), query=list("txt"=text))
r
http_status(r)
r <- POST(url, add_headers("X-Mashape-Key"= "b046W1AuLimshPLIBGkMKeHGwYw0p1RCYLJjsnyHjhot6bIwCy",
"Content-Type"="application/x-www-form-urlencoded",
"Accept"="application/json"))
r
headers(r)
r <- POST(url, add_headers("X-Mashape-Key"= "b046W1AuLimshPLIBGkMKeHGwYw0p1RCYLJjsnyHjhot6bIwCy",
"Content-Type"="application/x-www-form-urlencoded",
"Accept"="application/json"), query=list("txt"="Hallo"))
r
r <- POST(url, add_headers("X-Mashape-Key"= "WLY4kF1wdWmshkH3OOAbK5xxmYesp10v8RCjsnszDvQ4oCuvbs",
"Content-Type"="application/x-www-form-urlencoded",
"Accept"="application/json"), query=list("txt"="Hallo"))
r
str(content(r))
install.packages("XML")
str(content(r))
r <- POST(url, add_headers("X-Mashape-Key"= "WLY4kF1wdWmshkH3OOAbK5xxmYesp10v8RCjsnszDvQ4oCuvbs",
"Content-Type"="application/x-www-form-urlencoded",
"Accept"="application/json"), query=list("txt"="Hallo"))
r
content(r, "text")
r <- POST(url, add_headers("X-Mashape-Key"= "WLY4kF1wdWmshkH3OOAbK5xxmYesp10v8RCjsnszDvQ4oCuvbs",
"Content-Type"="application/x-www-form-urlencoded",
"Accept"="application/json"), query=list("txt"="Hallo"), accept_json())
r
content(r, "text")
r <- POST(url, add_headers("X-Mashape-Key"= "WLY4kF1wdWmshkH3OOAbK5xxmYesp10v8RCjsnszDvQ4oCuvbs",
"Content-Type"="application/x-www-form-urlencoded",
"Accept"="application/json"), query=list("txt"="Hallo bad bad bad"), accept_json())
content(r, "text")
r <- POST(url, add_headers("X-Mashape-Key:"= "WLY4kF1wdWmshkH3OOAbK5xxmYesp10v8RCjsnszDvQ4oCuvbs",
"Content-Type:"="application/x-www-form-urlencoded",
"Accept:"="application/json"), query=list("txt"="Hallo bad bad bad"), accept_json())
r
content(r, "text")
source(paste0(dir, "/Code/ExploratoryDataAnalysis.R"))
createUsefulPlots(trainingset, numericVariables, categoricVariables)
r
content(r, "text")
r <- POST(url, add_headers("X-Mashape-Key:"= "b046W1AuLimshPLIBGkMKeHGwYw0p1RCYLJjsnyHjhot6bIwCy",
"Content-Type:"="application/x-www-form-urlencoded",
"Accept:"="application/json"), query=list("txt"="Hallo bad bad bad"), accept_json())
content(r, "text")
url <- "https://community-sentiment.p.mashape.com/text/"
r <- POST(url, add_headers("X-Mashape-Key"= "b046W1AuLimshPLIBGkMKeHGwYw0p1RCYLJjsnyHjhot6bIwCy",
"Content-Type:"="application/x-www-form-urlencoded",
"Accept:"="application/json"), query=list("txt"="Hallo bad bad bad"), accept_json())
install.packages("JSON")
http_status(r)
str(content(r))
content(r, "text")
r <- GET(url, add_headers("X-Mashape-Key:"="b046W1AuLimshPLIBGkMKeHGwYw0p1RCYLJjsnyHjhot6bIwCy",
"Content-Type:"="application/x-www-form-urlencoded",
"Accept:"="application/json"), query=list("txt"="Hallo bad bad bad"), accept_json())
r
content(r, "text")
text
analysis[1:5]
analysis
help(ceils)
??ceils
help(ceil)
??ceil
getLiftMeasure<-function(y, yhat){
amount <- ceil(length(yhat)/10)
setwd("~/Documents/HU Berlin/WI 1516/BADS/Aufgabe/BADS")
dir<-getwd()
source(paste0(dir, "/Code/Utils.R"))
source(paste0(dir, "/Code/PlotHelper.R"))
#Script to install and load needed packages
source(paste0(dir, "/Code/Init.R"))
#Load Data
source(paste0(dir, "/Code/DataLoader.R"))
trainingset = getTrainigset(dir)
numericVariables = getNumericVariables(trainingset)
categoricVariables <- trainingset[setdiff(colnames(trainingset), colnames(numericVariables))]
continousVariablesname <- getContinousset(dir)
#Exploratory Data Analysis
source(paste0(dir, "/Code/ExploratoryDataAnalysis.R"))
#createUsefulPlots(trainingset, numericVariables, categoricVariables)
print("Loaded Dataset")
##Missing Value Handling
source(paste0(dir,"/Code/missingValueHandler.R"))
#trainingset <- getImputedData(trainingset)
#numericVariables = getNumericVariables(trainingset)
#categoricVariables <- trainingset[setdiff(colnames(trainingset), colnames(trainingset))]
#write.csv(trainingset, paste0(dir, "/Data/ImputedData.csv"), sep = ",")
trainingset <- loadImputedTrainingset(paste0(dir, "/Data/ImputedData.csv"))
numericVariables = getNumericVariables(trainingset)
categoricVariables <- trainingset[setdiff(colnames(trainingset), colnames(numericVariables))]
print("Finished Missing Value Handling")
trainingset <- trainingset[1:500,]
#Outlier Handling
source(paste0(dir, "/Code/Outliers.R"))
#z-score one-dimentional outlier handling
trainingset_withoutOutlier<- handle.Outliers.for.Matrix(trainingset)
print("Finished Outlier Handling")
#Data scaling with z-score
source(paste0(dir, "/Code/scaling.R"))
#traingsset
trainingset <- z.scale.data(m=trainingset,continous.var=continousVariablesname)
#traingsset_withoutOutlier
trainingset_withoutOutlier<- z.scale.data(m=trainingset_withoutOutlier,continous.var=continousVariablesname)
print("Finished Scaling")
#Corelation
#identify highly corelated coplete veriables
source(paste0(dir, "/Code/Correlation.R"))
data<-trainingset_withoutOutlier
trainingset_withoutCorrelated<-handle.highly.correlated.for.Matrix(data, .75,
which(colnames(data)=="Customer_ID"|colnames(data)=="churn"))
#####when skeaping the correlation
trainingset_withoutCorrelated<-trainingset_withoutOutlier
#identify highly corelated coplete veriables (only numeric)
#correlationMatrix <- cor(trainingset)
#correlationMatrix2 <- cor(trainingset_withoutOutlier)
##summary(correlationMatrix[upper.tri(correlationMatrix)])
# find attributes that are highly corrected (ideally >0.75)
#highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.90, verbose = FALSE)
#highlyCorrelated2 <- findCorrelation(correlationMatrix2, cutoff=0.90, verbose = FALSE)
#delete highly corelated columns
#trainingset<-trainingset[,-highlyCorrelated]
#trainingset_withoutOutlier<-trainingset_withoutOutlier[,-highlyCorrelated2]
#Feature selection
# source(paste0(dir, "/Code/FeatureSelection.R"))
# new dataset only containing selected features
selectedFeatures <- getSelectedFeatureSet(dir)
selectedFeatures <- c(as.vector(selectedFeatures[,1]), "churn")
#Split to test/trainigsset
trainingset <- trainingset[,selectedFeatures]
#trainingset_withoutOutlier <- trainingset_withoutOutlier[,selectedFeatures]
columns <-colnames(trainingset_withoutCorrelated)
#nicht alle selected features sind auch in trainingset_withoutCorrelated daher:
selectedFeatures_for_withoutCorrelated<-selectedFeatures[selectedFeatures %in% colnames(trainingset_withoutCorrelated)]
trainingset_withoutCorrelated_selecterFeatures <- trainingset_withoutCorrelated[,selectedFeatures_for_withoutCorrelated]
trainRandomForest <- function(data.tr){
rfGrid <- expand.grid(mtry=seq(5,70,5))
rf.tune <- train(churn~., data = data.tr, method="parRF", trControl = ctrl, tuneGrid=rfGrid)
return(rf.tune)
}
rf <- trainRandomForest(data.tr)
source(paste0(dir, "/Code/Utils.R"))
source(paste0(dir, "/Code/PlotHelper.R"))
source(paste0(dir, "/Code/Init.R"))
rf <- trainRandomForest(data.tr)
library("caret")
rf <- trainRandomForest(data.tr)
#Split to test/trainigsset
idx.train <- createDataPartition(y = trainingset$churn, p=0.7, list=FALSE)
data.tr <- trainingset[idx.train,]
data.ts <- trainingset[-idx.train,]
rf <- trainRandomForest(data.tr)
ctrl <- trainControl(method="cv", number = 20, classProbs = TRUE)
rf <- trainRandomForest(data.tr)
source(paste0(dir, "/Code/ModelTrainer.R"))
errorRates.nnet <- c()
errorRates.naiveBayes <- c()
errorRates.lr <- c()
errorRates.rf <- c()
errorRates.knn <- c()
errorRates.svm <- c()
errorRates.J48 <- c()
errorRates.ensemble <- c()
errorRates.ensemble_wo <- c()
rf <- trainRandomForest(data.tr)
idx.train <- createDataPartition(y = trainingset$churn, p=0.7, list=FALSE)
data.tr <- trainingset[idx.train,]
data.ts <- trainingset[-idx.train,]
data.tr
rf <- trainRandomForest(data.tr)
trainNaiveBayes <- function(data.tr){
nbGrid <- expand.grid(laplace=c(0,1), useKernel=c("TRUE", "FALSE"))
bayes.tune <- train(churn~., data = data.tr, method="nb", trControl = ctrl, trace=TRUE)
return(bayes.tune)
}
naiveBayes <- trainNaiveBayes(data.tr)
bayes.tune
naiveBayes
yhat.naiveBayes <- predict(naiveBayes, newdata = data.ts, type="raw")
yhat.naiveBayes
err.naiveBayes <-  ModelPerformanceByClass(data.ts$churn, yhat.naiveBayes)
err.naiveBayes
getLiftMeasure<-function(y, yhat){
amount <- ceil(length(yhat)/10)
yhat_sorted <- sort(yhat, decreasing = TRUE, index.return=1)
inds <- yhat_sorted$ix[1:amount]
liftMeasure <- (sum(as.numeric(data.ts[inds,]$churn=="bad"))/amount)/0.49562
return(liftMeasure)
}
yhat.naiveBayes <- predict(naiveBayes, newdata = data.ts, type="")
yhat.naiveBayes <- predict(naiveBayes, newdata = data.ts, type="prob")
yhat.naiveBayes
yhat.naiveBayes <- predict(naiveBayes, newdata = data.ts, type="raw")
yhat.naiveBayes
getLiftMeasure(data.ts$churn,yhat.naiveBayes)
getLiftMeasure<-function(y, yhat){
amount <- ceiling(length(yhat)/10)
yhat_sorted <- sort(yhat, decreasing = TRUE, index.return=1)
inds <- yhat_sorted$ix[1:amount]
liftMeasure <- (sum(as.numeric(data.ts[inds,]$churn=="bad"))/amount)/0.49562
return(liftMeasure)
}
getLiftMeasure(data.ts$churn,yhat.naiveBayes)
yhat.naiveBayes <- predict(naiveBayes, newdata = data.ts, type="probs")
yhat.naiveBayes <- predict(naiveBayes, newdata = data.ts, type="prob")
err.naiveBayes <-  ModelPerformanceByClass(data.ts$churn, yhat.naiveBayes)
getLiftMeasure(data.ts$churn,yhat.naiveBayes)
yhat.naiveBayes <- predict(naiveBayes, newdata = data.ts, type="raw")
y<-data.ts$churn
yhat<-yhat.naiveBayes
amount <- ceiling(length(yhat)/10)
amount
(length(yhat)/10)
yhat_sorted <- sort(yhat, decreasing = TRUE, index.return=1)
yhat_sorted
yhat.naiveBayes <- predict(naiveBayes, newdata = data.ts, type="prob")
amount <- ceiling(length(yhat)/10)
y<-data.ts$churn
yhat<-yhat.naiveBayes
yhat<-yhat.naiveBayes[2]
amount <- ceiling(length(yhat)/10)
yhat_sorted <- sort(yhat, decreasing = TRUE, index.return=1)
yhat
yhat_sorted <- sort(yhat, decreasing = TRUE)
yhat<-yhat.naiveBayes
yhat_sorted <- sort(yhat, decreasing = TRUE, index.return=1)
yhat<-yhat.naiveBayes$bad
yhat_sorted <- sort(yhat, decreasing = TRUE, index.return=1)
inds <- yhat_sorted$ix[1:amount]
yhat_sorted$ix
yhat_sorted
data.ts[inds,]$churn
data.ts
data.ts[inds,]$churn
liftMeasure <- (sum(as.numeric(data.ts[inds,]$churn=="bad"))/amount)/0.49562
liftMeasure
liftMeasure <- (sum(as.numeric(data.ts[inds,]$churn=="bad"))/amount)/0.5
liftMeasure
as.numeric(data.ts[inds,]$churn=="bad"
)
amount
amount <- ceiling(length(yhat)/10)
amount
as.numeric(data.ts[inds,]$churn=="bad"))/amount
liftMeasure <- (sum(as.numeric(data.ts[inds,]$churn=="bad"))/amount)/0.49562
liftMeasure
yhat_sorted <- sort(yhat, decreasing = TRUE, index.return=1)
inds <- yhat_sorted$ix[1:amount]
liftMeasure <- (sum(as.numeric(data.ts[inds,]$churn=="bad"))/amount)/0.49562
liftMeasure
data.ts[inds,]$churn=="bad"
as.numeric(data.ts[inds,]$churn=="bad"))/
data.ts[inds,]$churn
as.numeric(data.ts[inds,]$churn=="bad")
sum(as.numeric(data.ts[inds,]$churn=="bad"))/amount
inds
yhat_sorted
data.ts[inds,]$churn
(sum(as.numeric(data.ts[inds,]$churn=="bad"))/amount)
source('~/Documents/HU Berlin/WI 1516/refugeestest/IS refugees/main.R')
